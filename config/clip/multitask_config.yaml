pipeline_project: !!str "DeepCORO_multitask"
base_checkpoint_path: !!str outputs
# Training parameters
epochs: !!int 25
num_workers: !!int 16
debug: !!bool false
use_amp: !!bool true
period: !!int 1
run_mode: !!str train
# Dataset parameters
data_filename: !!str reports/dataset_with_splits_20250801.csv
root: !!str "."
target_label: !!str Report
datapoint_loc_label: !!str FileName
frames: !!int 16
stride: !!int 2
aggregate_videos_tokens: !!bool false # We need token-level features for captioning
per_video_pool: !!bool false
multi_video: !!bool false
num_videos: !!int 1
groupby_column: !!str StudyInstanceUID
shuffle_videos: !!bool true
batch_size: !!int 12 # Reduced batch size for multitask training
# Seed
seed: !!int 42
# Model parameters
model_name: !!str mvit
pretrained: !!bool true
# Optimizer parameters
optimizer: !!str AdamW
scheduler_name: !!str cosine
lr: !!float 0.00006171328778901703
lr_step_period: !!int 20
factor: !!float 0.4476789624289505
video_weight_decay: !!float 0.0000064829152813021765
text_weight_decay: !!float 7.969368788945428e-7
gradient_accumulation_steps: !!int 1
num_warmup_percent: !!float 0.1
num_hard_restarts_cycles: !!float 1.0
warm_restart_tmult: !!int 2
max_grad_norm: !!float 1.0
# Model architecture parameters 
num_heads: !!int 8
aggregator_depth: !!int 2
temperature: !!float 0.05881384886977135
dropout: !!float 0.15848373640344304
video_freeze_ratio: !!float 0.8704820438649167
text_freeze_ratio: !!float 0.9200933214042454
# Captioning decoder parameters
vocab_size: !!int 30522
decoder_layers: !!int 6
decoder_heads: !!int 8
decoder_intermediate_size: !!int 2048
max_position_embeddings: !!int 512
use_biomed_tokenizer: !!bool true
max_text_length: !!int 512
max_generation_length: !!int 128
captioning_do_sample: !!bool false
captioning_temperature: !!float 1.0
# Masked video modeling parameters
mvm_decoder_hidden_size: !!int 256
mvm_decoder_layers: !!int 2
mvm_decoder_heads: !!int 8
mask_ratio: !!float 0.75
mask_token_learnable: !!bool true
norm_predict_loss: !!bool true
# Learning rates for different components
text_lr: !!float 0.00002
captioning_lr: !!float 0.00006171328778901703
captioning_weight_decay: !!float 0.01
mvm_lr: !!float 0.000006171328778901703
mvm_weight_decay: !!float 0.01
# Loss configuration
loss_name: !!str multitask
contrastive_loss_type: !!str sigmoid # "sigmoid" or "softmax"
captioning_loss_type: !!str cross_entropy
masked_modeling_loss_type: !!str mse
label_smoothing: !!float 0.1
ignore_index: !!int -100
# Loss weights
loss_weights:
  contrastive: !!float 1.0
  captioning: !!float 1.0
  masked_modeling: !!float 0.1
  distillation: !!float 0.0
# Loss weight scheduler (optional)
use_loss_weight_scheduler: !!bool false
initial_loss_weights:
  contrastive: !!float 1.0
  captioning: !!float 0.5
  masked_modeling: !!float 0.1
final_loss_weights:
  contrastive: !!float 1.0
  captioning: !!float 1.0
  masked_modeling: !!float 0.1
loss_warmup_steps: !!int 1000
loss_total_steps: !!int 10000
loss_schedule_type: !!str linear # "linear", "cosine", "step"
# Checkpointing
resume_training: !!bool false
checkpoint: !!str ""
output_dir: !!str outputs
save_best: !!str loss
# Metrics
# Recall @k
recall_k: [1, 5, 10, 50]
# NDCG @k
ndcg_k: [5]
# Data augmentation
rand_augment: !!bool true
resize: !!int 224
apply_mask: !!bool false
# wandb parameters
tag: !!str mvit_multitask
name: !!str DeepCORO_Multitask_Training
project: !!str DeepCORO_Multitask
entity: !!str mhi_ai
use_wandb: !!bool true
# Inference parameters
topk: !!int 5
text_embeddings_path: !!str /media/data1/datasets/ECG_Tokenizer/utils/inference/reports_embeddings.pt
metadata_path: !!str /media/data1/datasets/ECG_Tokenizer/utils/inference/reports_metadata.parquet
inference_results_path: !!str checkpoints/inference/
