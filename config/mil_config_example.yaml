# Example Configuration for Multi-Instance Linear Probing

# Required base parameters
base_checkpoint_path: "" # Leave empty if not using a base checkpoint
name: "mil_probing_example"
project: "DeepCORO-MIL"
entity: "mhi_ai"
use_wandb: false
head_linear_probing: {} # Empty dict for no linear probing constraints
head_dropout: # Dropout rates for each head
  prox_rca_stenosis: 0.1
  mid_rca_stenosis: 0.1
  lad_stenosis: 0.1
# Inherit from a base config if available, otherwise define all needed fields
# defaults:
#   - override /base_config: heartwise_base # Example inheritance

# --- Project Settings ---
run_name: mil_probing_run_01
pipeline_project: DeepCORO_video_linear_probing # Matches ProjectRegistry/RunnerRegistry name
output_dir: ./outputs/${run_name}
run_mode: train # train or inference
device: cuda # or cpu
seed: 42
world_size: 1 # Set > 1 for DDP
# --- Dataset Parameters ---
data_filename: data/reports/reports_with_alpha_separator_with_Calcifc_Stenosis_IFR_20250507_STUDYLEVEL.csv # CSV with video paths and labels
datapoint_loc_label: FileName 
groupby_column: StudyInstanceUID
target_label: ["prox_rca_stenosis", "mid_rca_stenosis", "lad_stenosis"] # List of target column names
num_workers: 8
batch_size: 16 # Adjust based on N and GPU memory
frames: 32
stride: 2
resize: 224
rand_augment: true
# --- Video Encoder Parameters ---
model_name: mvit # Example backbone
aggregator_depth: 2
num_heads: 8 # Encoder attention heads, not prediction heads
video_freeze_ratio: 0.5 # Example: Freeze first 50% of the encoder
dropout: 0.1
pretrained: true
video_encoder_checkpoint_path: outputs/dev_deep_coro_clip_single_video/mvit_pretrained_mvit_b24_f16_AdamW_lr2.527361715636149e-05_20250325-001727_xvwwv5ar/checkpoints/best_epoch.pt
video_encoder_lr: 1e-5 # LR for unfrozen encoder parts
video_encoder_weight_decay: 1e-6
# --- Multi-Instance Linear Probing Parameters ---
pooling_mode: "attention" # 'mean', 'max', or 'attention'
attention_hidden: 256 # Hidden dim for attention mechanism
dropout_attention: 0.1 # Dropout within attention block
attention_lr: 5e-5 # Specific LR for attention parameters
attention_weight_decay: 1e-6 # Specific WD for attention parameters
# --- Head & Loss Structure ---
# Define prediction heads (name: num_classes)
head_structure:
  prox_rca_stenosis: 1 # Regression for proximal RCA stenosis
  mid_rca_stenosis: 1 # Regression for mid RCA stenosis
  lad_stenosis: 1 # Regression for LAD stenosis
# Define loss for each head (must match LossRegistry names)
loss_structure:
  prox_rca_stenosis: "mse"
  mid_rca_stenosis: "mse"
  lad_stenosis: "mse"
# Define task type for metrics
head_task:
  prox_rca_stenosis: "regression"
  mid_rca_stenosis: "regression"
  lad_stenosis: "regression"
# Learning rates per head
head_lr:
  prox_rca_stenosis: 5e-4
  mid_rca_stenosis: 5e-4
  lad_stenosis: 5e-4
# Weight decay per head
head_weight_decay:
  prox_rca_stenosis: 0.0
  mid_rca_stenosis: 0.0
  lad_stenosis: 0.0
# Weights for combining head losses
head_weights:
  prox_rca_stenosis: 1.0
  mid_rca_stenosis: 1.0
  lad_stenosis: 1.0
# Dropout per head (applied *after* pooling, before final layer)
# This 'head_dropout' might need to be added to the MIL model if desired
# head_dropout: 
#   prox_rca_stenosis: 0.0
#   mid_rca_stenosis: 0.0
#   lad_stenosis: 0.0

# (Deprecated for MIL model, use dropout_attention instead if needed)
# head_linear_probing: {}

# --- Training Parameters ---
optimizer: AdamW
scheduler_name: cosine_with_warmup # Example scheduler
epochs: 50
use_amp: true
gradient_accumulation_steps: 1
num_warmup_percent: 0.1
num_hard_restarts_cycles: 1.0
warm_restart_tmult: 1
lr_step_period: 10 # Only for step schedulers
factor: 0.1 # Only for step schedulers
# --- Logging/Misc ---
wandb:
  project: DeepCORO-MIL
  entity: mhi_ai # Optional
  log_freq: 100 # Log metrics every N steps
  log_code: true
# Label mappings for confusion matrices (Optional)
labels_map: {}
