pipeline_project: "DeepCORO_clip"
base_checkpoint_path: outputs
# ------------------------------------------------------------------
# Core training setup
# ------------------------------------------------------------------
run_mode: train
epochs: 30
num_workers: 10
debug: false
use_amp: true
period: 1
seed: 42
# Distributed / logging metadata
name: "siglip_ddp_mvit_f16"
project: "dev_deep_coro_clip_single_video"
entity: "mhi_ai"
tag: "siglip_ddp"
use_wandb: true
# ------------------------------------------------------------------
# Data parameters
# ------------------------------------------------------------------
data_filename: "output_dataset/siglip_generated/videos.csv"
root: "."
target_label: null
datapoint_loc_label: "FileName"
frames: 16
stride: 1
aggregate_videos_tokens: false
per_video_pool: false
multi_video: false
num_videos: 1
groupby_column: "StudyInstanceUID"
shuffle_videos: true
batch_size: 20 # per-device batch (DDP will multiply by world size)
# Normalisation used for cached frames
data_mean: [105.67617, 105.67617, 105.67617]
data_std: [38.953922, 38.953922, 38.953922]
# ------------------------------------------------------------------
# SigLIP specifics
# ------------------------------------------------------------------
siglip_texts_path: "output_dataset/siglip_generated/texts.csv"
siglip_max_positive_per_video: 8
siglip_negatives_per_video: 32
siglip_round_robin_sampling: true
siglip_max_segments_per_video: 15
siglip_positive_severity_weights:
  normal: 1.0
  mild: 1.5
  moderate: 2.5
  severe: 3.0
  critical: 3.0
  cto: 3.0
siglip_enable_severity_weighting: true
siglip_positive_loss_weight: 1.0
siglip_negative_loss_weight: 1.0
# ------------------------------------------------------------------
# Model definition
# ------------------------------------------------------------------
model_name: "mvit"
pretrained: true
video_freeze_ratio: 0.8
text_freeze_ratio: 0.75
dropout: 0.12
num_heads: 16
aggregator_depth: 1
# ------------------------------------------------------------------
# Optimisation + scheduler
# ------------------------------------------------------------------
optimizer: "AdamW"
lr: 0.00002
video_weight_decay: 1.0e-5
text_weight_decay: 1.0e-7
max_grad_norm: 1.0
video_max_grad_norm: 1.0
text_max_grad_norm: 1.0
gradient_accumulation_steps: 1
loss_name: "INFONCE_LOSS_DDP"
scheduler_name: "linear_warmup"
lr_step_period: 15
factor: 0.146
num_warmup_percent: 0.1
num_hard_restarts_cycles: 1
warm_restart_tmult: 2
# ------------------------------------------------------------------
# Contrastive / retrieval metrics
# ------------------------------------------------------------------
temperature: 0.087
recall_k: [1, 5, 10, 50]
ndcg_k: [5]
# ------------------------------------------------------------------
# Augmentation / preprocessing
# ------------------------------------------------------------------
rand_augment: true
resize: 224
apply_mask: false
# ------------------------------------------------------------------
# Checkpoint & inference paths
# ------------------------------------------------------------------
resume_training: false
checkpoint: ""
output_dir: outputs
save_best: "loss"
topk: 15
text_embeddings_path: "/media/data1/datasets/ECG_Tokenizer/utils/inference/reports_embeddings.pt"
metadata_path: "/media/data1/datasets/ECG_Tokenizer/utils/inference/reports_metadata.parquet"
inference_results_path: "checkpoints/inference/"
