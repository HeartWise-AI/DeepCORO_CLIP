{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64298/441343652.py:9: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pd.set_option('display.height', 1000)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def df_stats(df):\n",
    "    from tabulate import tabulate\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n***** Shape: \", df.shape, \" *****\\n\")\n",
    "\n",
    "        columns_list = df.columns.values.tolist()\n",
    "        isnull_list = df.isnull().sum().values.tolist()\n",
    "        isunique_list = df.nunique().values.tolist()\n",
    "        dtypes_list = df.dtypes.tolist()\n",
    "\n",
    "        list_stat_val = list(zip(columns_list, isnull_list, isunique_list, dtypes_list))\n",
    "        df_stat_val = pd.DataFrame(list_stat_val, columns=[\"Name\", \"Null\", \"Unique\", \"Dtypes\"])\n",
    "        print(tabulate(df_stat_val, headers=\"keys\", tablefmt=\"psql\"))\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64298/696543681.py:3: DtypeWarning: Columns (5,6,11,24,28,34,35,39,44,46,47,50,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,249,250,253,254) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_predictions = pd.read_csv(csv_file_path)\n"
     ]
    }
   ],
   "source": [
    "# Load the specified CSV file\n",
    "csv_file_path = \"/media/data1/ravram/DeepCORO/processed_dataframes/ObjectRecon_SWIN3D_2016-2023_inference_predictions_with_df_metadata_and_report.csv\"\n",
    "df_predictions = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    /media/data1/ravram/MHI_CATH_DICOM_VIDEOS/2022/2.16.124.113611.1.118.1.1.5994023_1.3.12.2.1107.5.4.5.135214.30000022010107492025500000000.dcm.avi\n",
       "Name: FileName, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_predictions.FileName.head(n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/925207 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "##############################################################################\n",
    "# 1) Base Vessel Definitions & Order                                         #\n",
    "##############################################################################\n",
    "labels_to_vessel_names = {\n",
    "    \"leftmain_stenosis\": \"the Left Main Coronary Artery (LMCA)\",\n",
    "    \"lad_stenosis\": \"the proximal LAD\",\n",
    "    \"mid_lad_stenosis\": \"the mid LAD\",\n",
    "    \"dist_lad_stenosis\": \"the distal LAD\",\n",
    "    \"D1_stenosis\": \"D1 branch\",\n",
    "    \"D2_stenosis\": \"D2 branch\",\n",
    "    \"D3_stenosis\": \"D3 branch\",\n",
    "    \"lcx_stenosis\": \"the proximal LCX\",\n",
    "    \"dist_lcx_stenosis\": \"the distal LCX\",\n",
    "    \"lvp_stenosis\": \"the LVP branch\",\n",
    "    \"marg_d_stenosis\": \"the marginal (Marg D) branch\",\n",
    "    \"om1_stenosis\": \"OM1\",\n",
    "    \"om2_stenosis\": \"OM2\",\n",
    "    \"om3_stenosis\": \"OM3\",\n",
    "    \"prox_rca_stenosis\": \"the proximal RCA\",\n",
    "    \"mid_rca_stenosis\": \"the mid RCA\",\n",
    "    \"dist_rca_stenosis\": \"the distal RCA\",\n",
    "    \"RVG1_stenosis\": \"RVG1\",\n",
    "    \"RVG2_stenosis\": \"RVG2\",\n",
    "    \"pda_stenosis\": \"the PDA\",\n",
    "    \"posterolateral_stenosis\": \"the posterolateral branch\",\n",
    "    \"bx_stenosis\": \"Ramus\",\n",
    "    \"lima_or_svg_stenosis\": \"the LIMA or SVG graft\",\n",
    "}\n",
    "\n",
    "vessel_order = [\n",
    "    \"leftmain_stenosis\",\n",
    "    \"lad_stenosis\",\n",
    "    \"mid_lad_stenosis\",\n",
    "    \"dist_lad_stenosis\",\n",
    "    \"D1_stenosis\",\n",
    "    \"D2_stenosis\",\n",
    "    \"D3_stenosis\",\n",
    "    \"lcx_stenosis\",\n",
    "    \"dist_lcx_stenosis\",\n",
    "    \"lvp_stenosis\",\n",
    "    \"marg_d_stenosis\",\n",
    "    \"om1_stenosis\",\n",
    "    \"om2_stenosis\",\n",
    "    \"om3_stenosis\",\n",
    "    \"prox_rca_stenosis\",\n",
    "    \"mid_rca_stenosis\",\n",
    "    \"dist_rca_stenosis\",\n",
    "    \"RVG1_stenosis\",\n",
    "    \"RVG2_stenosis\",\n",
    "    \"pda_stenosis\",\n",
    "    \"posterolateral_stenosis\",\n",
    "    \"bx_stenosis\",\n",
    "    \"lima_or_svg_stenosis\",\n",
    "]\n",
    "\n",
    "##############################################################################\n",
    "# 2) Short Formatting Helpers                                                #\n",
    "##############################################################################\n",
    "\n",
    "def format_stenosis_value(percent: float) -> str:\n",
    "    if percent == 0:\n",
    "        return \"no significant stenosis\"\n",
    "    elif 0 < percent < 50:\n",
    "        return f\"mild stenosis (~{percent}%)\"\n",
    "    elif 50 <= percent < 70:\n",
    "        return f\"moderate stenosis (~{percent}%)\"\n",
    "    elif 70 <= percent < 90:\n",
    "        return f\"severe stenosis (~{percent}%)\"\n",
    "    else:\n",
    "        return f\"critical stenosis (~{percent}%)\"\n",
    "\n",
    "def format_calcification_value(calcif: str) -> str:\n",
    "    txt = calcif.lower()\n",
    "    if \"no calcification\" in txt or \"pas de calcification\" in txt:\n",
    "        return \"no calcifications\"\n",
    "    elif \"minimes\" in txt or \"mild\" in txt:\n",
    "        return \"minimal calcifications\"\n",
    "    elif \"modérées\" in txt or \"moderate\" in txt:\n",
    "        return \"moderate calcifications\"\n",
    "    elif \"importantes\" in txt or \"severe\" in txt:\n",
    "        return \"severe calcifications\"\n",
    "    return f\"calcifications: '{calcif}'\"\n",
    "\n",
    "def format_ifr_value(ifr: float) -> str:\n",
    "    ifr_str = f\"{ifr:.2f}\"\n",
    "    if ifr > 0.89:\n",
    "        return f\"IFR normal (~{ifr_str})\"\n",
    "    return f\"IFR abnormal (~{ifr_str})\"\n",
    "\n",
    "##############################################################################\n",
    "# 3) Main Report Function with Custom Rules                                  #\n",
    "##############################################################################\n",
    "\n",
    "def create_report(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Builds a single short report per row using:\n",
    "    - Coronary dominance rules\n",
    "    - Graft presence rules\n",
    "    - Combined line per vessel (stenosis, calcif, IFR)\n",
    "    \"\"\"\n",
    "    # 1) Determine dominance and graft presence\n",
    "    dom_raw = str(row.get(\"coronary_dominance\", \"\"))\n",
    "    dom_lower = dom_raw.lower()\n",
    "    # Only show graft vessels if 'pontage' in Conclusion (case-insensitive)\n",
    "    # or bypass_graft == 1\n",
    "    conclusion_text = str(row.get(\"Conclusion\", \"\")).lower()\n",
    "    has_graft = (\"pontage\" in conclusion_text) or (row.get(\"bypass_graft\", 0) == 1)\n",
    "\n",
    "    # 2) Build a local copy of the vessel order we will actually iterate over\n",
    "    local_order = vessel_order[:]\n",
    "\n",
    "    # If Right Dominant: skip lvp & marg_d\n",
    "    if \"right\" in dom_lower:\n",
    "        if \"lvp_stenosis\" in local_order:\n",
    "            local_order.remove(\"lvp_stenosis\")\n",
    "        if \"marg_d_stenosis\" in local_order:\n",
    "            local_order.remove(\"marg_d_stenosis\")\n",
    "\n",
    "    # If not Left Dominant, keep default naming;\n",
    "    # if left dominant, rename 'pda_stenosis' and 'posterolateral_stenosis'\n",
    "    # to \"LEFT PDA\" and \"LEFT posterolateral\"\n",
    "    vessel_dict = labels_to_vessel_names.copy()\n",
    "    if \"left\" in dom_lower:\n",
    "        vessel_dict[\"pda_stenosis\"] = \"the LEFT PDA\"\n",
    "        vessel_dict[\"posterolateral_stenosis\"] = \"the LEFT posterolateral branch\"\n",
    "\n",
    "    # If no graft presence, remove lima_or_svg_stenosis\n",
    "    if not has_graft:\n",
    "        if \"lima_or_svg_stenosis\" in local_order:\n",
    "            local_order.remove(\"lima_or_svg_stenosis\")\n",
    "\n",
    "    # 3) Build each vessel’s text\n",
    "    lines = []\n",
    "    for stenosis_label in local_order:\n",
    "        prefix = stenosis_label.replace(\"_stenosis\", \"\")\n",
    "        vname = vessel_dict.get(stenosis_label, stenosis_label)\n",
    "\n",
    "        # Gather info from columns\n",
    "        desc = []\n",
    "        # Stenosis\n",
    "        st_val = row.get(stenosis_label, -1)\n",
    "        if pd.notna(st_val) and st_val != -1:\n",
    "            desc.append(format_stenosis_value(float(st_val)))\n",
    "        # Calcif\n",
    "        calc_label = prefix + \"_calcif\"\n",
    "        calc_val = row.get(calc_label, \"-1\")\n",
    "        if isinstance(calc_val, str) and calc_val.strip() != \"-1\":\n",
    "            desc.append(format_calcification_value(calc_val))\n",
    "        # IFR\n",
    "        ifr_label = prefix + \"_IFRHYPEREMIE\"\n",
    "        ifr_val = row.get(ifr_label, -1)\n",
    "        if pd.notna(ifr_val) and ifr_val != -1:\n",
    "            desc.append(format_ifr_value(float(ifr_val)))\n",
    "\n",
    "        # If we got any descriptors, combine in one short sentence\n",
    "        if desc:\n",
    "            # If multiple descriptors, separate by commas, last with 'and'\n",
    "            if len(desc) == 1:\n",
    "                combined = desc[0]\n",
    "            else:\n",
    "                combined = \", \".join(desc[:-1]) + \", and \" + desc[-1]\n",
    "            lines.append(f\"{vname} has {combined}.\")\n",
    "\n",
    "    # 4) Add coronary dominance if not empty\n",
    "    if dom_raw:\n",
    "        lines.append(f\"The coronary circulation is {dom_raw}.\")\n",
    "\n",
    "    # Return final or default\n",
    "    final_report = \"\\n\".join(lines)\n",
    "    if not final_report.strip():\n",
    "        return \"No significant findings or additional data available.\"\n",
    "    return final_report\n",
    "\n",
    "##############################################################################\n",
    "# 4) Example Usage                                                           #\n",
    "##############################################################################\n",
    "df_predictions[\"Report\"] = df_predictions.progress_apply(create_report, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_file_path = \"data/reports/reports_with_alpha_separator_with_Calcifc_Stenosis_IFR_20250108.csv\"\n",
    "output_dir = os.path.dirname(output_file_path)\n",
    "\n",
    "# Check if the directory exists, if not create it\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# Drop rows where 'External_Exam' is True\n",
    "df_predictions = df_predictions[df_predictions[\"External_Exam\"] != True]\n",
    "\n",
    "df_non_nan_reports = df_predictions.dropna(subset=[\"Report\"])\n",
    "# Filter the dataframe to keep only rows where 'object_value' is 5 or 9\n",
    "df_non_nan_reports = df_non_nan_reports[df_non_nan_reports[\"object_value\"].isin([5, 9])]\n",
    "\n",
    "\n",
    "df_non_nan_reports.to_csv(output_file_path, sep=\"α\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 833 characters\n",
      "Length: 832 characters\n",
      "Length: 944 characters\n",
      "Length: 832 characters\n",
      "Length: 832 characters\n",
      "Length: 832 characters\n",
      "Length: 831 characters\n",
      "Length: 831 characters\n",
      "Length: 832 characters\n",
      "Length: 831 characters\n",
      "Length: 831 characters\n",
      "Length: 831 characters\n",
      "Length: 831 characters\n"
     ]
    }
   ],
   "source": [
    "# Get top 13 reports and print their lengths\n",
    "top_13_reports = df_non_nan_reports.Report.value_counts().head(13)\n",
    "for report in top_13_reports.index:\n",
    "    print(f\"Length: {len(report)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73277/142835430.py:2: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
      "  df_non_nan_reports = pd.read_csv(output_file_path, sep=\"α\")\n"
     ]
    }
   ],
   "source": [
    "output_file_path = \"data/reports/reports_with_alpha_separator_with_Calcifc_Stenosis_IFR_20250108.csv\"\n",
    "df_non_nan_reports = pd.read_csv(output_file_path, sep=\"α\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73277/1124539113.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final = df_top5.groupby('StudyInstanceUID').apply(pick_values).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# First, sort the dataframe by 'series_time' and group by 'StudyInstanceUID'\n",
    "df_sorted = df_non_nan_reports.assign(series_time=pd.to_numeric(df_non_nan_reports.series_time, errors='coerce')).sort_values(by='series_time')\n",
    "\n",
    "# Group by 'StudyInstanceUID' and keep the top 5 earliest 'series_time'\n",
    "df_top5 = df_sorted.groupby('StudyInstanceUID').head(5)\n",
    "\n",
    "# Define a function to pick 3 rows with object_value == 5 and 2 rows with object_value == 9\n",
    "def pick_values(group):\n",
    "    # Filter rows where object_value == 5 and keep 3\n",
    "    group_5 = group[group['object_value'] == 5].head(3)\n",
    "    # Filter rows where object_value == 9 and keep 2\n",
    "    group_9 = group[group['object_value'] == 9].head(2)\n",
    "    # Concatenate the results\n",
    "    return pd.concat([group_5, group_9])\n",
    "\n",
    "# Apply the function to each group\n",
    "df_final = df_top5.groupby('StudyInstanceUID').apply(pick_values).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object_value\n",
       "5    88883\n",
       "9    39333\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_final.object_value.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Split\n",
       "train    115365\n",
       "val       12851\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split 70% of patients for train and 30% for validation\n",
    "unique_patients = df_final[\"CathReport_MRN\"].drop_duplicates()\n",
    "train_size = int(0.9 * len(unique_patients))\n",
    "train_patients = unique_patients.sample(n=train_size, random_state=42)\n",
    "val_patients = unique_patients.drop(train_patients.index)\n",
    "\n",
    "# Keep only the sampled patients in the dataframe\n",
    "df_sampled = df_final[\n",
    "    df_final[\"CathReport_MRN\"].isin(train_patients)\n",
    "    | df_final[\"CathReport_MRN\"].isin(val_patients)\n",
    "]\n",
    "\n",
    "# Sample 300 unique StudyInstanceUID from the already split dataset\n",
    "#unique_study_ids = df_sampled[\"StudyInstanceUID\"].drop_duplicates().sample(n=300, random_state=42)\n",
    "\n",
    "# Keep only the sampled StudyInstanceUIDs in the dataframe\n",
    "#df_sampled = df_sampled[df_sampled[\"StudyInstanceUID\"].isin(unique_study_ids)]\n",
    "\n",
    "# Assign split based on CathReport_MRN\n",
    "df_sampled.loc[df_sampled[\"CathReport_MRN\"].isin(train_patients), \"Split\"] = \"train\"\n",
    "df_sampled.loc[df_sampled[\"CathReport_MRN\"].isin(val_patients), \"Split\"] = \"val\"\n",
    "\n",
    "# Save the dataframe with the sampled StudyInstanceUIDs to a new CSV file\n",
    "output_sampled_file_path = \"data/reports/reports_with_alpha_separator_with_Calcifc_Stenosis_IFR_20250108.csv\"\n",
    "df_sampled.to_csv(output_sampled_file_path, sep=\"α\", index=False)\n",
    "\n",
    "display(df_sampled.Split.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sampled_file_path = pd.read_csv(\n",
    "    \"data/reports/reports_sampled_no_conclusion.csv\", sep=\"α\"\n",
    ")\n",
    "df_sampled = output_sampled_file_path.sample(96).reset_index()\n",
    "# Save the dataframe with split information to a new CSV file\n",
    "output_sampled_file_path = \"data/reports/reports_sampled_no_conclusion_96.csv\"\n",
    "df_sampled.to_csv(output_sampled_file_path, sep=\"α\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = pd.read_csv('data/reports/reports_with_alpha_separator_with_conclusion_and_more_details_20250108.csv', sep='α')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load PubMedBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "# Get a sample text from the dataframe\n",
    "sample_text = df_sampled['Report'].iloc[0]\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer(\n",
    "    sample_text,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Decode back to text to verify\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print full texts without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"\\nOriginal text (full):\")\n",
    "print(sample_text)\n",
    "print(\"\\nDecoded text (full):\")\n",
    "print(decoded)\n",
    "print(\"\\nEncoded tokens:\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
