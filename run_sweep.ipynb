{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: phhg8isv\n",
      "Sweep URL: https://wandb.ai/mhi_ai/DeepCORO_Clip_Sweep_Learnable_Temp_Full/sweeps/phhg8isv\n",
      "wandb: Agent Starting Run: c02btz3q with config:\n",
      "\tbatch_size: 48\n",
      "\tepochs: 10\n",
      "\tfactor: 0.1\n",
      "\tlearning_rate: 0.00015399987753409323\n",
      "\tlr_step_period: 10\n",
      "\toptimizer: AdamW\n",
      "\trandom_augment: True\n",
      "\tscheduler_type: cosine\n",
      "\ttemperature: 0.3626511179180052\n",
      "{'batch_size': 48, 'learning_rate': 0.00015399987753409323, 'epochs': 10, 'num_workers': 24, 'gpu': 1, 'model_name': 'mvit', 'optimizer': 'RAdam', 'weight_decay': 1e-06, 'scheduler_type': 'cosine', 'lr_step_period': 10, 'factor': 0.1, 'frames': 16, 'pretrained': True, 'config': 'config/default_config.yaml', 'local_rank': 0, 'debug': False, 'temperature': 0.3626511179180052, 'data_filename': 'data/reports/reports_sampled_no_conclusion.csv', 'root': '.', 'target_label': 'Report', 'datapoint_loc_label': 'FileName', 'stride': 2, 'random_augment': True, 'use_amp': True, 'project': 'deepCORO_CLIP', 'entity': 'mhi_ai', 'tag': 'DeepCORO_Clip_Sweep_Learnable_Temp_Full', 'output_dir': 'outputs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: WARNING Ignoring project 'deepCORO_CLIP' when running a sweep.\n",
      "wandb: WARNING Ignoring entity 'mhi_ai' when running a sweep.\n",
      "wandb: Currently logged in as: robertavram (mhi_ai). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.0\n",
      "wandb: Run data is saved locally in /volume/DeepCORO_CLIP/wandb/run-20241218_170507-c02btz3q\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run DeepCORO_Clip_Sweep_Learnable_Temp_Full\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/mhi_ai/DeepCORO_Clip_Sweep_Learnable_Temp_Full\n",
      "wandb: üßπ View sweep at https://wandb.ai/mhi_ai/DeepCORO_Clip_Sweep_Learnable_Temp_Full/sweeps/phhg8isv\n",
      "wandb: üöÄ View run at https://wandb.ai/mhi_ai/DeepCORO_Clip_Sweep_Learnable_Temp_Full/runs/c02btz3q\n",
      "wandb: WARNING Ignoring project 'deepCORO_CLIP' when running a sweep.\n",
      "wandb: WARNING Ignoring entity 'mhi_ai' when running a sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args:  Namespace(config='config/default_config.yaml', gpu=1, batch_size=48, num_workers=24, epochs=10, learning_rate=0.00015399987753409323, local_rank=0, debug=False, temperature=0.3626511179180052, data_filename='data/reports/reports_sampled_no_conclusion.csv', root='.', target_label='Report', datapoint_loc_label='FileName', frames=16, stride=2, random_augment=True, model_name='mvit', pretrained=True, optimizer='RAdam', weight_decay=1e-06, scheduler_type='cosine', lr_step_period=10, factor=0.1, use_amp=True, project='deepCORO_CLIP', entity='mhi_ai', tag='DeepCORO_Clip_Sweep_Learnable_Temp_Full', output_dir='outputs')\n",
      "{'batch_size': 48, 'learning_rate': 0.00015399987753409323, 'epochs': 10, 'num_workers': 24, 'gpu': 1, 'model_name': 'mvit', 'optimizer': 'RAdam', 'weight_decay': 1e-06, 'scheduler_type': 'cosine', 'lr_step_period': 10, 'factor': 0.1, 'frames': 16, 'pretrained': True, 'config': 'config/default_config.yaml', 'local_rank': 0, 'debug': False, 'temperature': 0.3626511179180052, 'data_filename': 'data/reports/reports_sampled_no_conclusion.csv', 'root': '.', 'target_label': 'Report', 'datapoint_loc_label': 'FileName', 'stride': 2, 'random_augment': True, 'use_amp': True, 'project': 'deepCORO_CLIP', 'entity': 'mhi_ai', 'tag': 'DeepCORO_Clip_Sweep_Learnable_Temp_Full', 'output_dir': 'outputs'}\n",
      "Args:  Namespace(config='config/default_config.yaml', gpu=1, batch_size=48, num_workers=24, epochs=10, learning_rate=0.00015399987753409323, local_rank=0, debug=False, temperature=0.3626511179180052, data_filename='data/reports/reports_sampled_no_conclusion.csv', root='.', target_label='Report', datapoint_loc_label='FileName', frames=16, stride=2, random_augment=True, model_name='mvit', pretrained=True, optimizer='AdamW', weight_decay=1e-06, scheduler_type='cosine', lr_step_period=10, factor=0.1, use_amp=True, project='deepCORO_CLIP', entity='mhi_ai', tag='DeepCORO_Clip_Sweep_Learnable_Temp_Full', output_dir='outputs')\n",
      "\n",
      "=== Calculating Dataset Statistics ===\n",
      "Stats dataset length: 128\n",
      "\n",
      "Using 100 samples for statistics calculation\n",
      "Frame count per video: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating statistics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:19<00:00,  6.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Mean: [121.43108367919922, 121.43108367919922, 121.43108367919922]\n",
      "Std:  [40.39977264404297, 40.39977264404297, 40.39977264404297]\n",
      "Calculated from 100 samples (80,281,600 pixels)\n",
      "===========================\n",
      "\n",
      "Using MViT backbone - forcing exactly 16 frames with stride=1\n",
      "\n",
      "Available splits in dataset:\n",
      "Split\n",
      "train    311970\n",
      "val       35529\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset loading statistics for split 'train':\n",
      "Total rows in CSV: 347499\n",
      "Valid files found: 347499\n",
      "Matching split 'train': 311970\n",
      "Final dataset size: 311970\n",
      "Tokenizer initialized successfully\n",
      "Using MViT backbone - forcing exactly 16 frames with stride=1\n",
      "\n",
      "Available splits in dataset:\n",
      "Split\n",
      "train    311970\n",
      "val       35529\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset loading statistics for split 'val':\n",
      "Total rows in CSV: 347499\n",
      "Valid files found: 347499\n",
      "Matching split 'val': 35529\n",
      "Final dataset size: 35529\n",
      "Tokenizer initialized successfully\n",
      "Frozen layers (VideoEncoder): []\n",
      "Unfrozen layers (VideoEncoder): []\n",
      "Initialized TextEncoder with:\n",
      "  model_name: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\n",
      "  hidden_size: 768\n",
      "  vocab_size: 30522\n",
      "  output_dim: 512\n",
      "Frozen layers (TextEncoder): ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "Unfrozen layers (TextEncoder): ['encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias']\n",
      "\n",
      "=== Dataset Information ===\n",
      "Training:   311,970 videos\n",
      "Validation: 35,529 videos\n",
      "Total:      347,499 videos\n",
      "\n",
      "Batch Size: 48\n",
      "Training Batches: 6,499\n",
      "Validation Batches: 741\n",
      "===========================\n",
      "\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|‚ñé         | 220/6499 [05:02<3:40:50,  2.11s/it, train_loss=3.8714, avg_train_loss=3.8720]502 response executing GraphQL.\n",
      "\n",
      "<html><head>\n",
      "<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
      "<title>502 Server Error</title>\n",
      "</head>\n",
      "<body text=#000000 bgcolor=#ffffff>\n",
      "<h1>Error: Server Error</h1>\n",
      "<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\n",
      "<h2></h2>\n",
      "</body></html>\n",
      "\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6499/6499 [2:24:20<00:00,  1.33s/it, train_loss=3.8715, avg_train_loss=3.8713]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss: 3.8713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 741/741 [16:39<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average text embedding (first 5 dims): tensor([ 0.0002, -0.0008,  0.0032,  0.0014, -0.0822], device='cuda:0')\n",
      "Validation Loss: 3.8689\n",
      "Saved checkpoint to outputs/deepCORO_CLIP/DeepCORO_Clip_Sweep_Learnable_Temp_Full_mvit_b48_f16_RAdam_lr0.00015399987753409323_20241218-170508_c02btz3q/checkpoints/latest.pt\n",
      "\n",
      "Saved latest checkpoint at epoch 1\n",
      "Saved checkpoint to outputs/deepCORO_CLIP/DeepCORO_Clip_Sweep_Learnable_Temp_Full_mvit_b48_f16_RAdam_lr0.00015399987753409323_20241218-170508_c02btz3q/checkpoints/best.pt\n",
      "\n",
      "New best model saved! Val Loss (val-only): 3.8689 (previous: inf)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|‚ñã         | 433/6499 [09:52<1:38:48,  1.02it/s, train_loss=3.8709, avg_train_loss=3.8712]502 response executing GraphQL.\n",
      "\n",
      "<html><head>\n",
      "<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
      "<title>502 Server Error</title>\n",
      "</head>\n",
      "<body text=#000000 bgcolor=#ffffff>\n",
      "<h1>Error: Server Error</h1>\n",
      "<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\n",
      "<h2></h2>\n",
      "</body></html>\n",
      "\n",
      "Training:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 5926/6499 [2:14:32<09:50,  1.03s/it, train_loss=3.8711, avg_train_loss=3.8712]  "
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "def load_yaml_config(file_path):\n",
    "    with open(file_path) as file:\n",
    "        return yaml.safe_load(file)\n",
    " \n",
    "\n",
    "sweep_conf_file_path = \"config/sweep_config.yaml\"\n",
    "sweep_conf = load_yaml_config(sweep_conf_file_path)\n",
    "count = 25  # number of runs to execute\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_conf, project=sweep_conf[\"name\"], entity=\"mhi_ai\")\n",
    "\n",
    "wandb.agent(sweep_id=sweep_id, entity=\"mhi_ai\", project=sweep_conf[\"name\"], count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
