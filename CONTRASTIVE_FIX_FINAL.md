# Final Fix: Make Multitask Contrastive Work Like CLIP

## Problem Summary

Your multitask training showed:
- ‚ùå Contrastive loss stuck/barely decreasing
- ‚ùå Negative alignment scores
- ‚ùå Poor retrieval performance
- ‚úÖ Good NLP metrics (ROUGE/BLEU >0.6) BUT missing stenosis percentages

## Root Causes Identified

### üî• Critical Issue #1: Gradient Interference from Patch-Level Contrastive

**The Problem**:
```python
# multitask_loss.py - TWO contrastive losses competing
contrastive_loss = loss_fn(video_features, text_features, log_temp)  # Study-level
patch_contrastive_loss = loss_fn(patch_pooled, text_features, log_temp)  # Patch-level (SAME text!)
```

**Why This Breaks Learning**:
- Text encoder receives conflicting gradients
- Must align with BOTH study-level AND patch-pooled features
- These two representations differ (aggregator vs mean-pooling)
- Result: Poor alignment, gradient noise, no learning

**Evidence**: CLIP doesn't use patch-level contrastive - only study-level!

### üî• Critical Issue #2: Loss Weights Prioritize Captioning Over Contrastive

**The Problem**:
```yaml
# OLD config (WRONG priority)
loss_weights:
  contrastive: 0.8    # Secondary
  captioning: 1.2     # PRIMARY (50% higher!)
  masked_modeling: 0.5
```

**Why This Breaks Learning**:
- Features optimized for sequential generation (captioning)
- Sequential generation ‚â† contrastive alignment
- Contrastive becomes a weak auxiliary task
- Result: No alignment learning

**Evidence**: CLIP has 100% contrastive focus - that's why it works!

### üî• Critical Issue #3: Temperature Too High (10.0 vs CLIP's 0.1)

**The Problem**:
```yaml
temperature: 10.0  # Very soft probabilities
```

**Why This Breaks Learning**:
- Soft probabilities ‚Üí weak gradients
- Weak gradients ‚Üí slow/no learning
- SigLIP at temp=10 designed for large-scale (billions of samples)
- Your dataset too small for soft temperature

**Evidence**: CLIP uses temp=0.0998, not 10.0!

### üêõ Issue #4: Confusing Temperature Initialization

**The Problem**:
```python
# Multitask (confusing syntax)
log_temperature = nn.Parameter(
    torch.tensor([torch.log(torch.tensor(temp))], ...)
)

# CLIP (clean syntax)
log_temperature = nn.Parameter(
    torch.log(torch.tensor([temp], ...))
)
```

**Why This Matters**: Confusing code ‚Üí harder to debug, not actually broken but misleading

---

## Fixes Applied

### ‚úÖ Fix #1: Disabled Patch-Level Contrastive

**File**: `config/clip/multitask_config.yaml`

```yaml
# Line 101 - BEFORE
patch_contrastive_weight: !!float 0.4

# Line 101 - AFTER
patch_contrastive_weight: !!float 0.0  # DISABLED
```

**File**: `runners/multitask_runner.py`

```python
# Line 836 - BEFORE
patch_features=video_tokens,

# Line 836 - AFTER
# patch_features removed
```

**Result**: Single contrastive objective, no gradient interference

---

### ‚úÖ Fix #2: Rebalanced Loss Weights (Contrastive Dominant)

**File**: `config/clip/multitask_config.yaml`

```yaml
# Lines 94-98 - BEFORE
loss_weights:
  contrastive: !!float 0.8
  captioning: !!float 1.2
  masked_modeling: !!float 0.5

# Lines 94-98 - AFTER
loss_weights:
  contrastive: !!float 2.0    # 2.5x increase ‚Üí DOMINANT
  captioning: !!float 0.3      # 4x decrease ‚Üí auxiliary
  masked_modeling: !!float 0.1  # 5x decrease ‚Üí weak regularization
```

**File**: `config/clip/sweep_config_multitask.yaml`

```yaml
# Lines 98-111 - BEFORE
loss_weights.contrastive: 0.5-1.0
loss_weights.captioning: 1.0-2.0
loss_weights.masked_modeling: 0.3-0.8

# Lines 98-111 - AFTER
loss_weights.contrastive: 1.5-3.0   # DOMINANT range
loss_weights.captioning: 0.2-0.5     # AUXILIARY range
loss_weights.masked_modeling: 0.05-0.2  # WEAK range
```

**Result**: Contrastive learning prioritized, features optimized for alignment

---

### ‚úÖ Fix #3: Reduced Temperature (Sharp Distribution)

**File**: `config/clip/multitask_config.yaml`

```yaml
# Line 55 - BEFORE
temperature: !!float 10.0

# Line 55 - AFTER
temperature: !!float 0.1
```

**File**: `config/clip/sweep_config_multitask.yaml`

```yaml
# Lines 27-35 - BEFORE
temperature_start: 5.0-20.0
temperature_end: 5.0-20.0

# Lines 27-35 - AFTER
temperature_start: 0.07-0.15  # CLIP range
temperature_end: 0.07-0.15    # CLIP range
```

**Result**: Sharp probabilities ‚Üí strong gradients ‚Üí fast learning

---

### ‚úÖ Fix #4: Cleaned Temperature Initialization

**File**: `projects/multitask_pretraining_project.py`

```python
# Lines 167-175 - BEFORE (confusing)
log_temperature = nn.Parameter(
    torch.tensor([torch.log(torch.tensor(self.config.temperature))], ...)
)

# Lines 167-175 - AFTER (clean, matches CLIP)
log_temperature = nn.Parameter(
    torch.log(torch.tensor([self.config.temperature], ...))
)
```

**Result**: Clean code, matches working CLIP implementation exactly

---

### ‚úÖ Fix #5: Increased Gradient Clipping

**File**: `config/clip/multitask_config.yaml`

```yaml
# Line 48 - BEFORE
max_grad_norm: !!float 0.5

# Line 48 - AFTER
max_grad_norm: !!float 1.0  # Match CLIP
```

**Result**: Slightly larger gradients allowed for stable training

---

## Expected Results

### Before Fix (Broken):
```
Epoch 1: contrastive=3.0, alignment=-0.2, Recall@1=0.05
Epoch 2: contrastive=2.9, alignment=-0.15, Recall@1=0.06
Epoch 3: contrastive=2.9, alignment=-0.1, Recall@1=0.07
...stuck...
```

### After Fix (Working):
```
Epoch 1: contrastive=2.5, alignment=0.15, Recall@1=0.20
Epoch 2: contrastive=2.0, alignment=0.30, Recall@1=0.35
Epoch 3: contrastive=1.7, alignment=0.42, Recall@1=0.48
...improving steadily...
```

### Key Metrics to Monitor:

1. **Contrastive Loss**:
   - ‚úÖ Should decrease steadily (not stuck)
   - Target: <1.5 by epoch 10

2. **Alignment Score**:
   - ‚úÖ Should be POSITIVE (not negative!)
   - Target: >0.3 by epoch 5

3. **Retrieval Performance**:
   - ‚úÖ Recall@1 should improve
   - Target: >0.4 by epoch 10

4. **Captioning Quality**:
   - ‚úÖ NLP metrics stay high (ROUGE/BLEU >0.6)
   - ‚úÖ NOW includes stenosis percentages!
   - Target: Generate "70%", "90%" stenoses

5. **Loss Balance**:
   - ‚úÖ Contrastive contributes most to total loss
   - Target: contrastive_loss * 2.0 > captioning_loss * 0.3

---

## Comparison: CLIP vs Multitask (Now Fixed)

| Component | Working CLIP | Broken Multitask | Fixed Multitask |
|-----------|-------------|------------------|-----------------|
| **Temperature** | 0.0998 | 10.0 ‚ùå | 0.1 ‚úÖ |
| **Contrastive weight** | 1.0 (100%) | 0.8 (31%) ‚ùå | 2.0 (67%) ‚úÖ |
| **Captioning weight** | N/A | 1.2 (46%) ‚ùå | 0.3 (10%) ‚úÖ |
| **Patch contrastive** | None | 0.4 ‚ùå | 0.0 ‚úÖ |
| **Gradient clipping** | 1.0 | 0.5 ‚ùå | 1.0 ‚úÖ |
| **Feature extraction** | Simple | Complex ‚ö†Ô∏è | Complex ‚ö†Ô∏è |
| **Temperature init** | Clean | Confusing ‚ö†Ô∏è | Clean ‚úÖ |

---

## How to Run

### Option 1: Test Single Config First

```bash
source .venv/bin/activate

# Run with fixed base config
bash scripts/runner.sh \
  --base_config config/clip/multitask_config.yaml \
  --selected_gpus 0,1,2 \
  --use_wandb true \
  --run_mode train
```

**Watch for**:
- Epoch 1: Alignment should be positive (>0.1)
- Epoch 2: Contrastive loss should decrease
- Epoch 3: Should see stenosis % in generated captions

If this works, proceed to sweep!

---

### Option 2: Run Full Sweep with Fixed Ranges

```bash
source .venv/bin/activate

bash scripts/run_sweep.sh \
  --base_config config/clip/multitask_config.yaml \
  --sweep_config config/clip/sweep_config_multitask.yaml \
  --selected_gpus 0,1,2 \
  --count 20
```

**Sweep will explore**:
- Contrastive weight: 1.5-3.0
- Captioning weight: 0.2-0.5
- MVM weight: 0.05-0.2
- Temperature: 0.07-0.15

---

## Debugging Tips

### If contrastive still doesn't learn:

1. **Increase contrastive weight further**:
   ```yaml
   loss_weights.contrastive: 3.0 ‚Üí 5.0
   ```

2. **Decrease temperature more**:
   ```yaml
   temperature: 0.1 ‚Üí 0.05
   ```

3. **Check feature dimensions**:
   ```python
   print(f"Video: {video_features.shape}")  # Should be [B, 512]
   print(f"Text: {text_features.shape}")    # Should be [B, 512]
   ```

4. **Monitor gradient norms**:
   ```bash
   # In W&B, check:
   grad_norm_video_backbone  # Should be 0.1-1.0
   grad_norm_text_encoder    # Should be 0.1-1.0
   ```

---

### If captioning quality drops:

1. **Slightly increase captioning weight**:
   ```yaml
   loss_weights.captioning: 0.3 ‚Üí 0.5
   ```

2. **Keep contrastive dominant** (‚â•2.0)

---

## Summary of All Changes

### Files Modified:

1. ‚úÖ `config/clip/multitask_config.yaml`
   - Temperature: 10.0 ‚Üí 0.1
   - Loss weights: contrastive=2.0, captioning=0.3, mvm=0.1
   - Patch weight: 0.4 ‚Üí 0.0
   - Gradient clipping: 0.5 ‚Üí 1.0

2. ‚úÖ `config/clip/sweep_config_multitask.yaml`
   - Temperature range: 5.0-20.0 ‚Üí 0.07-0.15
   - Contrastive weight: 0.5-1.0 ‚Üí 1.5-3.0
   - Captioning weight: 1.0-2.0 ‚Üí 0.2-0.5
   - MVM weight: 0.3-0.8 ‚Üí 0.05-0.2
   - Removed patch_contrastive_weight sweep

3. ‚úÖ `projects/multitask_pretraining_project.py`
   - Temperature init: Cleaned syntax to match CLIP

4. ‚úÖ `runners/multitask_runner.py`
   - Removed `patch_features=video_tokens` argument

---

## Key Takeaway

**The core problem**: Trying to be too clever with patch-level contrastive and auxiliary task weighting.

**The solution**: **BE LIKE CLIP** - Keep it simple:
- ‚úÖ Single contrastive objective (study-level only)
- ‚úÖ Contrastive dominant (2.0 weight)
- ‚úÖ Sharp temperature (0.1)
- ‚úÖ Auxiliary tasks stay auxiliary (<0.5 weight)

**Why this works**: CLIP works because it focuses on ONE thing (alignment) and does it well. Multitask can work by keeping the SAME focus but adding auxiliary tasks that SUPPORT (not compete with) the primary objective.

---

## References

- Working CLIP: `projects/contrastive_pretraining_project.py`
- Working runner: `runners/video_constrative_learning_runner.py`
- CLIP config: `config/clip/base_config.yaml` (temperature ‚âà 0.1)
- Original SigLIP paper: Uses temp=10 for **billions** of samples
- Your dataset: ~2000 samples ‚Üí needs CLIP-style sharp temp

**Good luck!** üöÄ
